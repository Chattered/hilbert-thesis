\chapter{Conclusion}\label{chapter:Conclusion}
In an article published in the \emph{American Mathematical Monthly}~\cite{GuggenheimerJordanProof}, on the validity of the Polygonal Jordan Curve Theorem in ordered geometry, almost forgotten, Guggenheimer laments:
\begin{quotation}
  ``It is astonishing that none of the textbooks of elementary axiomatic geometry gives a proof.''
\end{quotation}

We find it more astonishing considering that the axioms of ordered geometry mark the first major contribution to the modern rigorisation of geometry by Pasch, ``the father of rigour in geometry''. It stretches all our credibility when those same axioms were delineated in the most influential textbook on modern axiomatic geometry, nearly eight decades earlier,  and the theorem stated as derivable, by David Hilbert, one of the greatest mathematicians who ever lived.

Such is hindsight. We confess that we had little interest in the theorem initially. Hilbert implied it was trivial, that, in the \emph{Grundlagen}, the interesting proofs only appear after the later groups of axioms. If not for the demands of formal verification, we would have missed the literal labyrinths that were left unexplored in the wake of his first two groups. We would have almost certainly glossed over the details, letting ourselves be convinced by invalid proofs --- Veblen's, our own, and whatever Hilbert had in mind when he declared the result obtainable without much difficulty. Had it not been for formal verification, we would not have done justice to ordered geometry.

However, the substantial effort required of a formal verification detracts from these benefits. With current technology, simply translating Hilbert's prose steps to formal logic and expecting generic automated tools to fill in all the logical inferences from a small logical kernel is unfeasible. The automation cannot fill the gaps in a timely fashion, possibly because synthetic geometry, in contrast to algebraic geometry, offers little opportunity for equational reasoning and is less amenable to contextual rewriting, a staple of proof automation. Even with unlimited time, it is possible that the automation would inevitably crash as it fills limited memory with doomed search strategies.

We were happy to persevere with the synthetic style of proof, but not happy with the prospect of painstakingly filling the gaps by hand, adding excruciatingly detailed intermediate steps. By the time the gaps are filled, the verification is a bloated and obscure take on the original prose and the geometric insight that produced the original is lost in the noise of fussy intermediate facts about which points lie or do not lie on which lines, justified by axioms that Hilbert did not deign to reference. 

Fortunately, it turned out that the implicitly used axioms could be handled with \emph{custom} automation. Since the axioms do not introduce new points into a geometric configuration, we can exhaustively search for all the incidence relations that hold in a figure. Restricting our attention to a small finite geometry in this way, characterised by sets of collinear, non-collinear and planar points, and the rules between them, we are faced with a tractable combinatorial space, and so we developed a tool to search it.

With  automation, we typically found that our new verifications corresponded one-to-one with the prose. That is, our verifications were structured just as if the formal steps were direct translations of steps in the prose argument, the ideal of declarative proofs. It is as if we had managed to recreate the silent mechanical reasoning capacity of an artificial Hilbert, going some way to vindicate the great mathematician's commitment to axiomatic rigour.

Moreover, it turned out to be crucial to have this automation by the time we came to verify the Polygonal Jordan Curve Theorem. We were trying to patch a proof clouded in doubt even under the scrutiny of accomplished mathematicians. We only dared the venture because we were confident that our incidence automation made it feasible. Otherwise, we could scarcely imagine how we would have completed it. The verification consisted of roughly 1400 declarative proof steps. Of these, 213 are simple \code{assume} steps that come with no justification. Another 215 are \code{consider} steps which introduce geometric entities. 116 are \code{fix} steps for naming the quantified variables. Of the remaining 849 intermediate steps, 111 were handled by the incidence automation of Chapter~\ref{chapter:Automation} and 82 by the combination of incidence automation and linear reasoning automation of Chapter~\ref{chapter:LinearOrder}. Going by the results of Chapter~\ref{chapter:Group2Eval}, we may have replaced over 500 proof steps about tedious incidence reasoning alone.

These are steps that had been difficult to obtain by hand. Some we had missed entirely, something we could expect to happen frequently on the substantially larger verification of the Polygonal Jordan Curve Theorem, leading to mental blocks which may then have sent us down spurious paths, or worse, had us give up entirely. But with the incidence details taken care of, our proofs could focus on the less combinatorial steps, those that interest a model geometer, and which can be sketched out on paper and subjected to the full resources of intuitive observation.

The tools could also be used in an exploratory fashion, running concurrently as we developed our verifications, delivering intermediate results according to a growing proof context. Here, we benefitted greatly from our implementation in a search algebra. Interdependent discoverers could be defined and then called on individually, or composed together with other discoverers, perhaps to perform transformations such as exhaustively splitting the  conjunctions and disjunctions they generated. We relied heavily on this flexibility as we explored each proof.

In the future, such flexibility should also allow us to tailor the automation to new domains, such as the systematic enumeration of equal angles in a diagram as we come to tackle such things in Group~III of the \emph{Grundlagen der Geometrie}. We would hope that the reasoning here could easily be expressed as a set of discoverers, and then combined with the incidence discoverers to create a powerful reasoning tool that is more than the sum of its parts. We expect there to be some interesting possibilities here for future work.

One lesson we would take from the need and efficacy of custom automation is that the ideal of declarative verification, where the user only has to think in terms of lemmas and their dependencies, letting generic automation connect the dots, is probably going to be overly optimistic for some time to come. Domain specific automation, crafted by the user, will still be needed, and those working in formal verification must continue to think as computer scientists and programmers. Theorem provers must continue to cater to their needs, as HOL~Light does, putting its users directly at the interpreter of a powerful programming language conceived with the explicit aim of writing proofs.

The varied landscape of theorem proving demands that any generic tools we build are highly extensible and composable. The powerful tactic languages~\cite{Tactics} characteristic of the LCF tradition~\cite{LCF} are a classic example of such tools. Our composable search algebras, implemented as combinators, follow such an example. Thus, they allowed us to rapidly prototype a search tool specific to incidence reasoning, without being limited to this narrow domain.

So far, we have only considered using the algebras as a way to represent breadth first searches, but they may not be limited to this. Conceivably, streams could be transformed by additional stream transformations which selectively delay data according to a suitable ordering, thereby favouring some search paths over others. By using such transformations, we might tackle more exotic search problems and perhaps try to implement classic search procedures such as those used in resolution provers.

There are a number of optimisation issues still to be dealt with before we think about pursuing this. We would need to find a way to deal with equality and normalisation, which would allow us to combine two independent searches that are generated from two terms or theorems with the same normal form. More generally, we would need to deal with the fact that a search path may be redundant because it is guaranteed to generate data that is weaker than data generated by another search path.

Finally, we would like to be more aggressive in how our trees are pruned. Our trees represent case-splits, and it is common in search that the children of a given node will each end up containing the same item of data. Those duplicate items should really be merged into a single item in the parent, since the implication is that the item is found in all cases. Similarly, there will be times when a contradiction is found in one of the children, or more generally, an item which is known to be stronger than any other possible item. In this case, the entire subtree can be discarded, eliminating the case entirely. With these optimisations, we can eliminate redundant data and duplicated searches.

Even without these improvements, we reiterate that the automation allowed us to carry out our verification without any grinding monotony. We found our proofs with pencil and paper, carefully working out geometric constructions, and then transcribed the details into HOL~Light without having to fill in many gaps. And when we came to review our proofs, it was trivial to reproduce those diagrams and reconstruct the argument in prose.

And this made the effort all the more rewarding, and we would like to conclude by reflecting a little on the experience. We believe that formal verification can hold a unique place in the general space of human problem solving. The chief point is that, in a verification, the goal is stated up front without ambiguity, and the circumstances under which that goal is achieved are completely unambiguous and authoritative. The sense of success in solving a problem is rarely so definitive and clear cut.

We had the problem stably formalised very early on, the prospect of its solution was almost tangible. We just needed the theorem prover to produce the formalised problem statement as a fully fledged theorem, curtly announcing ``No subgoals''. However, we did not anticipate the effort required to get there. This was not because the verification was a slog, for our automation took care of the pedantic mechanical details. It was just that the problem seemed to grow in complexity as we tackled it, and the theorem prover could not give us a reliable measure of our distance from the goal.

We could sweat out a major lemma such as \ref{eq:changeTriangle} from Appendix~\ref{app:JordanVerificationExtra}, but with theorems such as these have half a dozen opaque hypotheses and based on admittedly convoluted definitions, we found ourselves with some doubt about over overall plan and the map we had charted. At any time, we were prepared to find ourselves having navigated ourselves into a dead-end and giving up entirely, having misunderstood some crucial aspect of the problem that had misled great mathematicians. And struggling ourselves to make sense of the problem, we found it very difficult to communicate what we were pinning our hopes on.

When the theorem prover finally and curtly announced ``No subgoals'', it was like turning a corner in the labryinth and then emerging disoriented in bright daylight. In an instant, we knew that we had solved the problem that had started it all, instantaneously vindicating our strategy and dissolving our previous uncertainties. The  They must be slowly eased by checking the method of solution, which is retained so that it can sooth the doubts of others. The subtle complexities of the solution must be magnified lest they mislead us into fallacy. But with the guarantees of a computer checked verification, subtleties can be accepted with some suspension of disbelief, details left only to tell a story in which a few nuggets in the form of useful techniques might be found for other adventurers. Doubts can never emerge from a closer reading. The sense of accomplishment is absolute and unambigous, an experience we suspect is unique in the sciences.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
