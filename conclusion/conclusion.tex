\chapter{Conclusion}\label{chapter:Conclusion}
In an article published in the \emph{American Mathematical Monthly}, on the validity of the Polygonal Jordan Curve Theorem in ordered geometry, almost forgotten, Guggenheimer laments:
\begin{quote} ``It is astonishing that none of the textbooks of elementary axiomatic geometry gives a proof.''
  \flushright{\cite[p. 753]{GuggenheimerJordanProof}}
\end{quote}

We find it more astonishing considering that the axioms of ordered geometry mark the first major contribution to the modern rigorisation of geometry by Pasch, ``the father of rigour in geometry''. It stretches all our credibility when those same axioms were delineated in the most influential textbook on modern axiomatic geometry, nearly eight decades earlier,  and the theorem stated as derivable, by David Hilbert, one of the greatest mathematicians who ever lived.

Such is hindsight. We confess that we had little interest in the theorem initially. Hilbert implied it was trivial, that, in the \emph{Grundlagen}, the interesting proofs only appear after the later groups of axioms. But he left literal labyrinths unexplored in the wake of those first two groups, and we would have passed them by if not for formal verification. It would have been easy to gloss over the details, to be convinced by invalid proofs --- Veblen's, our own, and whatever Hilbert had in mind when he declared the result obtainable without much difficulty. Had that happened, we would not have done justice to ordered geometry.

But the details we must attend to in formal verification are, frankly, excessive. With current technology, we cannot simply translate even highly rigorous prose steps to formal logic and expect generic automated tools to fill in the missing inferences, certainly not in a timely fashion.

This problem was compounded for us, we conjecture, because we chose a domain which was synthetic, not algebraic. Here, there is little room for the equational reasoning that characterises algebraic proofs, and where powerful term rewriting systems can be made the workhorse of automation. Instead, we have implicational theorems with complex antecedents, and too many variables which need explicit instantiation.

Happily, as we saw in Chapter~\ref{chapter:Group2Eval}, it turned out that the domain was amenable to a tailored proof-search. Since the inferences needed to fill the proof gaps almost never introduce points into a geometric configuration, we can exhaustively search for all the incidence relations that hold in a figure. Restricting our attention to a small finite geometry in this way, characterised by sets of collinear, non-collinear and planar points, and the rules between them, we were faced with a tractable combinatorial space.

It is possible that there are other theories for which we can carve out such a space, and which have so far resisted formalisation for lack of ways to tailor a search of it. By following the example of traditional theorem proving languages~\cite{Tactics}, we have shown how a combinator language could be used to rapidly prototype the appropriate search strategies using a very modest amount of declarative code. As an additional benefit, the strategies, themselves built by composition rules, are naturally extensible and composable across domains. They could potentially scale well to ever more complex problems.

A crucial assumption of our search language is that the data found (such as sequents about which sets of points are collinear) grows cumulatively. Later data are assumed not to replace earlier data, and no data are discarded as new information comes in. These are not generally realistic assumptions, and already posed some inefficiences for us when we came to think about point equalities. Such data should ideally force all prior data to be normalised.

Nevertheless, the tool was highly effective at dealing with incidence arguments, and with it, we typically found that our new verifications corresponded one-to-one with the prose. That is, our verifications were structured just as if the formal steps were direct translations of steps in the prose argument. This ideal is rarely achieved in declarative proof, and gives us new hope that formal verification can one day play a major role in ordinary mathematics. It is as if we had managed to recreate the silent mechanical reasoning capacity of an artificial Hilbert, allowing us to write a much more \emph{human} verification without worrying about slacking on rigour.

It was crucial to have this automation by the time we came to verify the Polygonal Jordan Curve Theorem. Typically, those working in formal verification transcribe proofs that they know to be basically correct. We, on the other hand, were trying to patch a proof that we were convinced was wrong. We only dared the venture because we were confident that our incidence automation made it feasible. Otherwise, we could scarcely imagine how we would have completed it.

The verification consisted of roughly 1400 declarative proof steps. Of these, 213 are simple \code{assume} steps that come with no justification. Another 215 are \code{consider} steps which introduce geometric entities. 116 are \code{fix} steps for naming the quantified variables. Of the remaining 849 intermediate steps, 111 were assisted by the incidence automation of Chapter~\ref{chapter:Automation} and 82 by the combination of incidence automation and linear reasoning automation of Chapter~\ref{chapter:LinearOrder}. Going by the results of Chapter~\ref{chapter:Group2Eval}, we may have replaced over 500 proof steps about tedious incidence reasoning alone.

These are steps that had been difficult to obtain by hand. Some we had missed entirely, something we could expect to happen frequently on the substantially larger verification of the Polygonal Jordan Curve Theorem, leading to mental blocks which may then have sent us down spurious paths, or worse, had us give up entirely. But with the incidence details taken care of, our proofs could focus on the less combinatorial steps, those that interest a model geometer, and which can be sketched out on paper and subjected to the full resources of intuitive observation.

One lesson we would take from the need and efficacy of our automation is that the current ideal of declarative verification, where the user only has to think in terms of lemmas and their dependencies, letting only \emph{generic} automation connect the dots, is probably going to be overly optimistic for some time to come. Domain specific automation, crafted by the user, will still be needed, and those working in formal verification must continue to think as computer scientists and programmers. Theorem provers must continue to cater to their needs, as HOL~Light does, putting its users directly at the interpreter of a powerful programming language so that they can craft new proof tools highly tailored to the problem at hand.

We would like to conclude by reflecting a little on our experience verifying the Polygonal Jordan Curve Theorem. We believe that formal verification holds a unique place in the general space of human problem solving. The chief point is that, in a verification, the goal is stated up front without any ambiguity, and the circumstances under which that goal is achieved are likewise unambiguous. The result is authoritative and not subject to any human review. 

We had the problem stably formalised very early on, and so the prospect of its solution was almost tangible. We just needed the theorem prover to produce the formalised problem statement as a fully fledged theorem. However, we did not anticipate the effort required to get there. This was not because the verification was a slog, for our automation took care of the pedantic mechanical details. It was just that the problem seemed to grow in complexity as we tackled it, and the theorem prover could not give us a reliable measure of our distance from the goal.

We could sweat out a major lemma such as \ref{eq:changeTriangle} from Appendix~\ref{app:JordanVerificationExtra}, but with theorems such as these, which have half a dozen opaque hypotheses and are based on admittedly convoluted definitions, we found it difficult to convince ourselves that the theorems said exactly what we wanted them to say and that they put us on the right track. 
At any time, we were prepared for the possibility that we were navigating ourselves into a dead-end, having misunderstood some crucial aspect of the problem. We struggled to find our bearings and aim a true course, and the effort required made it difficult to communicate any sense of real progress to others. We were quite alone in this maze.

So when the theorem prover finally and curtly announced ``No subgoals'', it was like turning yet another dull corner and then suddenly emerging disoriented in bright daylight. There was no need to trace back over our tracks, scrutinising and carefully reassuring ourselves that we had indeed found the exit. In an instant, we knew that we had solved the problem that had started it all, that our strategy was vindicated and all previous uncertainties had evaporated.

In practised mathematics, a proof must convince human readers. The subtle complexities of its solution must be teased out and magnified lest they mislead us into fallacy. But a verification is wholly unlike this. Already convinced that our verifier is sound, and having been long convinced that the relatively short problem statement is the one we intend, the verification is complete the moment the computer declares it so. Looking back over the verification, we can start accepting the subtleties with some suspension of disbelief, and not feel obliged to investigate every last nook and cranny: the machine has vouched for us. The path of definitions and lemmas can instead be left as a story offering only insight on the complexities involved, along with some nuggets of useful techniques and strategies for other adventurers.

Our normal scintillas of doubt reflect the diligence due in rigorous problem solving, arising from a sense of personal responsibility for our errors and a desire to be honest and not mislead readers. Such doubts would rightly beg us to revisit and elaborate our proofs from time-to-time. With verification, they are only misplaced. Even the most rigorous of axiomatic geometers must envy the resulting sense of a task so definitively complete.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
