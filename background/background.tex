\chapter{Background}\label{chapter:Background}
In this chapter, we describe our chosen logic and our chosen verification tool, \linebreak HOL~Light~\cite{HOLLight}. The chapter is provided to keep the thesis self-contained, and contains very little in the way of new material. To assist the reader wanting to skip topics familiar or otherwise irrelevant to them, we describe each section.

In \S\ref{sec:ObjectLogic}, we give a brief overview of simple type theory. In \S\ref{sec:ObjectLogicFormal}, we give its formal definition.

In \S\ref{sec:LCF}, we review the LCF approach to implementing interactive proof assistants, and our chosen proof assistant HOL~Light.

In \S\ref{sec:DeclarativeProof}, we introduce the idea of a declarative verification, and in \S\ref{sec:MizarLight}, we discuss the Mizar~Light language that we have used. In \S\ref{sec:MizarLightExtend}, we present previously unpublished material on some useful extensions and modifications to the Mizar~Light language.

\section{Object Logic}\label{sec:ObjectLogic}
The logic of our proof assistant is Church's Simple Theory of Types~\cite{ChurchTheoryOfTypes} or the simply-typed lambda calculus. This calculus is typically associated with the foundations of programming languages, but it was originally conceived as a foundation for logic~\cite{UntypedTheoryLambdaCalculus}, one which avoided the paradoxes of the n\"{a}ive set theories in a way that was less ``artificial'' than both ZF set theory and Russell's Ramified Type Theory~\cite{RussellTheoryOfTypes}.

The original lambda calculus turns out to be inconsistent (all terms are provably\linebreak equal~\cite{InconsistencyLambdaCalculus}), and ironically, Church fixed the problem in essentially the same way as Russell, though by the time he did so, Russell's ramified types had been simplified; hence, the \emph{simple theory of types}.

There might be some concern that the use of lambda calculus is unfaithful to mathematics which is nominally based on first-order set theory, but practised verification usually favours typed higher-order logics. Russell and Whitehead's celebrated computer unassisted verification~\cite{Principia} was in a typed higher-order logic. The first non-trivial computer assisted verifications used DeBruijn's AUTOMATH~\cite{AUTOMATH} and a powerful extension of the simply typed lambda calculus. And according to Wiedijk's survey~\cite{SeventeenProvers}, eleven of the world's seventeen proof assistants were based on a higher-order logic as of 2006. Even those based nominally on untyped set theory such as Mizar~\cite{MizarMathematicalVernacular} can be viewed as typed~\cite{MizarSoftTypes}.

The definitions to follow include a standard simple extension to Church's original theory. In Church's formalism, there are theorems and proofs which hold for any substitution of their types, and which were stated as schemas. Rather than use schemas, we make substitutable types part of the syntax, by introducing \emph{type variables}. Theorems (and terms generally) which feature these type variables are called \emph{polymorphic} and replace the theorem schemas of the original calculus. This makes computerised provers based on simple type theory more efficient since polymorphic theorems have only one proof to verify while a schematic theorem must be verified for each type we want to apply it to. The expressive power of the logic is preserved since types can only be instantiated and never generalised.

\subsection{Definitions}\label{sec:ObjectLogicFormal}
We now give a formal definition of the object logic.

\subsubsection{Syntax}
First of all, we introduce \emph{types}, which can be understood as disjoint collections of values and which can be defined from an alphabet of type constants and an alphabet of type variables, such that
\begin{enumerate}
\item every type constant and every type variable is a type;
\item for types $\tau_1$ and $\tau_2$, we have that $\tau_1 \rightarrow \tau_2$ is a (function) type. Arrow composition is right-associative, so that $\alpha \rightarrow \beta \rightarrow \gamma$ parses as $\alpha \rightarrow (\beta \rightarrow \gamma)$.
\end{enumerate}

Next, we have \emph{terms} based on an alphabet of term constants and term variables. All terms have a unique type, given by the typing relation $(:)$. If we say that terms denote values, then we can say that the typing relation associates a term with the type of its value. The rules are:
\begin{enumerate}
\item Every term constant and every term variable is a term.
\item For terms $f\ :\ \tau_1\rightarrow\tau_2$ and $x\ :\ \tau_1$, we have that $f\ x$ is a term such that \mbox{$f\ x : \tau_2$}. These \emph{combinations} are left-associative: $f\ g\ h = (f\ g)\ h$.
\item Given a term variable $x : \tau_1$ and a term $y : \tau_2$, there is a term $\lambda x. y : \tau_1 \rightarrow \tau_2$. These \emph{lambda abstractions} consume everything to the right, so $\lambda x. f\ x\ y$ parses as $\lambda x. ((f\ x)\ y)$.
\end{enumerate}

The idea is that the type $\alpha \rightarrow \beta$ is inhabited by functions with domain $\alpha$ and codomain $\beta$. A lambda abstraction in this type is then a function literal, and the notation $\lambda x. f\ x$ can be understood as the familiar mathematical notation $x \mapsto f\ x$.

All functions in simple type theory have arity 1. If we want an $n$-ary function with \mbox{$n>1$}, we typically map a single argument to another function which expects the remaining $n-1$ arguments. So a two argument function from $\alpha$ and $\beta$ to $\gamma$ is given type $\alpha \rightarrow \beta \rightarrow \gamma$. This is known as \emph{currying}, and its pervasiveness explains why function application is left-associative in simple type theory. We want to write a two-argument function application as $f\ x\ y$, and not the more cumbersome $(f\ x)\ y$.

The syntax unifies several ideas from first-order logic. Rather than having a separate syntax for formulas and terms, we just declare that formulas \emph{are} terms but in a designated type \code{bool} of truth values . We can then take predicates, sets, functions and relations to be higher-order functions with codomain \code{bool}. Thus, a predicate is now just a function of type $\alpha \rightarrow \code{bool}$ and a binary relation $\alpha \times \beta$ is a function of type $\alpha \rightarrow \beta \rightarrow \code{bool}$. Sets are represented by predicates (their characteristic functions), and by using functions from predicates to predicates, we can recover simple set-theoretic operations such as union and intersection.

Types restrict the set of terms that could otherwise be generated by combination and lambda abstraction, and prevents the paradox which motivated Russell and Whitehead's logic. That paradox asks us to consider the set of all sets which do not contain themselves. When we understand a set $X$ as some predicate $X:U \rightarrow \code{bool}$, and the assertion $X \not\in X$ as the negation $\neg(X\ X)$, we would formalise Russell's paradox as $\lambda X. \neg(X\ X)$. But this is not well-typed, since the $X$ cannot be given a type consistent with rules~2 and~3.

\subsubsection{Calculus}\label{sec:HOLInferenceRules}
The calculus for simple type theory used in HOL~Light is based on the primitive type of truth values $\code{bool}$ and the primitive (polymorphic) relation $(=)\ :\ \alpha \rightarrow \alpha \rightarrow \code{bool}$. With these, we can introduce the notion of \emph{judgements} or \emph{sequents}, which have the form
\begin{displaymath}
\{A_1:\code{bool},A_2,\ldots,A_n\} \vdash C\ :\ \code{bool}.
\end{displaymath}
We are to understand these sequents as saying that $C$ is judged true in the context of assumptions $\{A_1,A_2,\ldots,A_n\}$. The sequents are linked by inference rules, which tell us how new sequents arise from existing sequents. In other words, we prove sequents, rather than propositions.

The rules of the calculus as used in HOL~Light (Figure~\ref{fig:HOLInferenceRules}) are few, and so far as we know, admit only one redundancy: the transitivity inference rule is provided as an optimisation. Each rule is expressed as a horizontal line, with sequents below the line being derivable from sequents above. We omit all types, since these can always be inferred from the terms. In fact, theorem provers based on this logic, including HOL~Light, will automatically infer the most general type of all terms~\cite{HindleyMilner}.

\begin{figure}
  \begin{gather*}
    \begin{aligned}
      \infer[\code{REFL}]{\vdash x = x}{} &\qquad&
      \infer[\code{TRANS}]{\Gamma\cup\Delta\vdash s=u}{\Gamma\vdash s=t&\Delta\vdash t=u}\\
      \infer[\code{MK\_COMB}]{\Gamma\cup\Delta\vdash f\ x=f\ y}{\Gamma\vdash x = y & \Delta \vdash f = g} &&
      \text{for any $x$ not free in $\Gamma$,}\quad\infer[\code{ABS}]{\Gamma \vdash (\lambda x. s) = \lambda x. t}{\Gamma \vdash s = t}\\
      \infer[\code{BETA}]{\vdash (\lambda x. t)\ x = t}{}&&
      \infer[\code{ASSUME}]{\{A\} \vdash A}{}
    \end{aligned}\\
    \infer[\code{EQ\_MP}]{\Gamma\cup\Delta\vdash Q}{\Gamma\vdash P = Q & \Delta\vdash P}\\
    \infer[\code{DEDUCT\_ANTISYM\_RULE}]{(\Gamma - \{Q\})\cup(\Delta - \{P\})\vdash P = Q}{\Gamma\vdash P & \Delta\vdash Q}\\
  \end{gather*}
  \caption{Inference Rules}
  \label{fig:HOLInferenceRules}
\end{figure}

There are two more rules to handle instantiations of free term variables and type variables. We have a rule to substitute terms $t_1$, $\ldots$, $t_n$ for term variables $x_1$, $\ldots$, $x_n$ (avoiding variable capture). We have another rule to substitute types $\tau_1$, $\ldots$, $\tau_n$ for type variables $\alpha_1$, $\ldots$, $\alpha_n$.
\begin{align*}
  \infer[\code{INST}]{\Gamma[t_1,\ldots,t_n]
    \vdash p[t_1,\ldots,t_n]}
  {\Gamma[x_1,\ldots,x_n]\vdash p[x_1,\ldots,x_n]}&&
  \infer[\code{INST\_TYPE}]{\Gamma[\alpha_1,\ldots,\alpha_n]
    \vdash p[t_1,\ldots,t_n]}
  {\Gamma[\tau_1,\ldots,\tau_n]\vdash p[\tau_1,\ldots,\tau_n]}
\end{align*}

We omit the formal definitions of substitution here. We just mention that $\lambda$ \emph{binds} free variables just like a quantifier does.

\subsection{Higher-order Logic}
With only equality and lambda abstraction, it might be surprising that the calculus embeds a full (intuitionistic) higher-order logic (hereafter \emph{HOL}). The definitions of all connectives and quantifiers are given in Table~\ref{table:HOL}, ordered by precedence rather than definitional dependency.

The logic is higher order since the variable $x$ in both quantifiers is polymorphic. Thus, we can quantify over any type, including predicates and functions. We can also quantify over booleans, a fact which is exploited in the definitions of disjunction and the existential quantifier.

All encodings are intuitionistic. In particular, notice that \mbox{$P \implies Q$} is not defined as $\neg P \vee Q$, and disjunction and existential quantification are not defined via the DeMorgan correspondences, as they might be in a classical logic.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$(p,q)$          & $(\lambda f. f\ p\ q)$                & Pairing\\
$\top$           & $(\lambda x. x) = \lambda x. x$       & Truth\\
$\bot$           & $\forall P. P$                        & Absurdity\\
$\neg P$         & $P = \bot$                            & Negation\\
$P \wedge Q$     & $(P,Q) = (\top,\top)$                 & Conjunction\\
$P \vee Q$       & $\forall R. (P \implies R) \implies (Q \implies R) \implies R$  &Disjunction\\
$P \implies Q$   & $P \iff P \wedge Q$                   & Implication\\
$P \iff Q$       & $P = Q$                               & Equivalence\\
$\forall x. P x$ & $(\lambda x. P\ x) = \lambda x. \top$ & Universal\\
$\exists x. P x$ & $\forall R. (\forall x. P\ x \implies R) \implies R$ & Existential\\
$\code{let}\ x=t\ \code{in}\ y$ & $(\lambda x. y)\ t$ & Local variable\\
&& definition\\
\hline
\end{tabular}
\end{center}
\caption{HOL Basics}
\label{table:HOL}
\end{table}

\section{Proof Assistant}\label{sec:LCF}
Our chosen proof assistant, HOL~Light, is implemented in the Ocaml programming language. The roots of both go back to the original statically typed functional language ML, and the LCF tradition~\cite{LCF}.

\subsection{Edinburgh LCF}
If natural languages serve as the metalanguages for defining object logics for human proof checkers, then ML serves as the metalanguage for defining the object logics of \emph{computerised} proof checkers. Accordingly, when we define the syntax of an object logic, ML understands ``inductive data type'' where we understand ``inductive definition'', and ML understands ``function from sequents to sequent'' where we understand ``inference rule''.

The use of a programming language can clarify a lot. Consider the definition of the $\code{ABS}$ inference rule in Figure~\ref{fig:HOLInferenceRules}, where we say ``for any $x$ not free in $\Gamma$'', a qualification which is absent from the definition of say, $\code{BETA}$. The ML code makes the difference here very clear: the function implementing the inference rule $\code{ABS}$ takes a variable $x$ as an additional argument, while that for $\code{BETA}$ does not.

Sequents and inference rules in HOL~Light, as in all LCF systems, are implemented in ML as a \emph{trusted kernel}. To gain the trust, a user can carefully peruse the kernel's modest 672 lines of ML, especially the 160 or so lines of code needed to deal with free variable substitutions, instantiations, and testing whether two terms are equivalent up to a consistent renaming of bound variables (formally, testing for ``$\alpha$-equivalence''). This code is tricky to reason about, just as it is tricky to define and reason about free-substitutability in basic proof theory. In fact, two bugs were identified and fixed in this part of the HOL~Light code circa 2003.

User code, again written in ML, calls the inference rules of the kernel in order to construct sequents and thereby mechanically verify theorems. All the mechanical verifications mentioned in this thesis consist of such code. However, it is rare that we use the kernel directly. Having a full programming language available to user code means that LCF systems typically offer powerful derived rules, embedded proof languages, and fully automated tools to assist in constructing sequents. HOL~Light is no exception, and we discuss some of its offerings in \S\ref{sec:UserTools}, and in Chapter~\ref{chapter:Automation}, we describe our own forward search tool.

We are not expected to exploit the implementation of sequents and inference rules, and indeed \emph{cannot}. The sequent data type is made \emph{abstract} to user code, and only the kernel's inference rules are visible. ML's strong type system guarantees that user code cannot break this abstraction. Any sequent constructed in user code is a sequent that was ultimately constructed legitimately via the kernel's inference rules.

\subsection{Additional Functionality}
HOL~Light extends the simple theory of types by allowing us to add new axioms~(see \S\ref{sec:ClassicalAxioms}) and add new term and type definitions. Definitions cannot contain free variables on their right hand sides, nor can they be recursive. Thus, they are a conservative extension. We could potentially substitute the right hand sides of any definitions through all terms without affecting the validity of the derivations.

HOL~Light also extends the simple theory of types by allowing one to define an abstract type of values in bijection with a non-empty subset of an existing type. This feature is particularly useful when forcing derived definitions to be used in a way which respects an abstraction.

For instance, in Chapter~\ref{chapter:HalfPlanes}, we define both ``rays'' and ``half-planes'' as abstract types in bijection with point-sets satisfying appropriate constraints. For rays, we define an ``endpoint'' in terms of the concrete representation, and can thus use expressions such as ``endpoint of a ray''. However, since the types are abstract, the expression ``endpoint of a half-plane'' is not well-typed, even though both rays and half-planes have the same concrete representation. The type \code{ray} enforces the intended abstraction. As a final benefit, because HOL Light types can always be mechanically reconstructed from expressions, abstract types can eliminate the need for some side-conditions by pushing the constraints on their concrete representations into an automatically inferred type.

The combination of type definitions and polymorphism leads to another desirable but simple extension. Types can be defined from polymorphic definitions, such as lists which are polymorphic in their element type. In this case, the type variables in the definition become the arguments of a \emph{type constructor}. The extension replaces the two rules of the type syntax with the single rule

\begin{itemize}
\item[] for every natural number $n$ (the \emph{arity}), type constructor $T$ and types $\tau_1, \tau_2, \ldots \tau_n$, we have that $T\ \tau_1\ \tau_2\ \ldots\ \tau_n$ is a type.
\end{itemize}

Type constants are now just type-constructors with arity-0, while $(\rightarrow)$ is just a type-constructor with arity-2. The type of lists is a type-constructor $\code{List}$ with arity 1, but from here on, we write $\code{List}\ \alpha$ as $[\alpha]$.

\section{Classical Logic}\label{sec:ClassicalAxioms}
HOL~Light allows any formula $A$ to be permanently asserted an axiom, giving the sequent $\vdash A$. We shall use this facility in the next chapter to define the axiomatics of Hilbert's geometry. The facility is also used in the standard distribution of the system to make HOL a classical logic.

A standard classical axiom added to HOL Light is the axiom of extensionality, or the $\eta$-reduction axiom. 
\begin{displaymath}
\forall t. (\lambda x. t\ x) = t.
\end{displaymath}
With it, one can show that $f = g$ precisely when $f\ x = g\ x$ for arbitrary $x$. Thus, functions are equal when they agree on their outputs, and sets are equal precisely when they have the same members.

The propositional fragment of HOL becomes classical with the introduction of the term $\epsilon$, whose sole axiom is:\footnote{Here, $\epsilon$ acts as a binder, like $\lambda$, $\forall$ and $\exists$, but this is syntax sugar. In reality, $\epsilon$ is a term of type $(\alpha \rightarrow \code{bool}) \rightarrow \alpha$.}
\begin{displaymath}
\forall P\ x.\ P\ x \implies P\ (\epsilon x. P\ x).
\end{displaymath}

We are to understand $\epsilon x. P$ as ``the arbitrarily chosen $x$ satisfying $P$''. This is the full axiom of choice, used frequently as a definitional tool. It should be contrasted with the weaker $\iota$ binder, which yields expressions $\iota x. P$ to be read as ``the uniquely specified $x$ satisfying $P$.'' The distinction is discussed further in \S\ref{sec:UseOfIota}.

The axiom governing this notion clarifies how simple type theory handles undefined or non-referring terms. Consider what happens when $P$ is unsatisfiable. In this case, $\epsilon x. P\ x$ is still, in a sense, well-defined. From the perspective of the logic's model theory, all terms, even peculiar ones such as $\epsilon x. P\ x$ where $P$ is unsatisfiable, must refer. But from a proof-theoretic point of view, since the condition on the axiom can never be discharged for this particular $P$, nothing interesting can ever be shown true of $\epsilon x. P\ x$. The only sequents we can derive of it are logical truths, and if we take Wittgenstein at his word in the \emph{Tractacus}~\cite{ToWitNothing}, we might say that when all we can derive are logical truths, we have nothing. It is in this sense that we can formalise the notion of undefined terms and the undefined values of partial functions in HOL. Each one is just $\epsilon x. \bot$.

The axiom of choice is sufficient to derive the law of excluded-middle, by exploiting the fact that $\epsilon$ terms with equivalent bodies are equal. In particular, we consider\linebreak $u = \epsilon x. x \vee P$ and $v = \epsilon x. \neg x \vee P$. Since we have $\top \vee P$ and $\neg \bot \vee P$, we obtain by the axiom of choice $(u \vee P) \wedge (\neg v \vee P)$ which is equivalent to
\begin{equation}
 (u \wedge \neg v) \vee P.\label{eq:AxiomChoiceDerive}
\end{equation}

Now on the hypothesis of $P$, we know that the $\epsilon$ terms are equal. That is,
\begin{displaymath}
P \implies u = v \implies \neg (u \wedge \neg v),
\end{displaymath}
or alternatively, $u \wedge \neg v \implies \neg P$. We then conclude from \eqref{eq:AxiomChoiceDerive} that $\neg P \vee P$.\footnote{The propositional inferences here are all intuitionistically valid.}

We end this subsection by introducing a final piece of core syntax. The axiom of choice allows us to encode a conditional operator as follows:
\begin{equation}
\code{if}\ b\ \code{then}\ x\ \code{else}\ y \equiv \epsilon z. (b \implies z = x) \wedge (\neg b \implies z = y) .
\end{equation}

\subsection{Axiom of Infinity}\label{sec:InfinityDescription}
The final axiom in HOL~Light is the axiom of infinity. We will discuss this axiom in some detail in Chapter~\ref{chapter:LinearOrder}. For now, we will only say in advance that it is provably redundant when we have Hilbert's first two groups of axioms. We have therefore removed it. It is tempting to draw a parallel here between this \emph{post-hoc} deletion of the axiom of infinity from higher-order logic with Hilbert's later expressed scepticism on the security of infinite sets~\cite{OnInfinite}: ``[the infinite] neither exists in nature nor provides a legitimate basis for rational thought.''

\section{Verification Tools}\label{sec:UserTools}
Outside of the relatively tiny HOL~Light kernel is a huge collection of verification tools written in user code. In many cases, we do not care how these tools work, or whether they even contain bugs, since bugs cannot penetrate the boundary of the kernel. We only care that they generate the sequent we want.

\subsection{Tactics}
HOL~Light implements the LCF tactic system~\cite{Tactics}, in which we understand the process of constructing a sequent as the solving of a goal by breaking it into subgoals. We will need to briefly discuss the implementation of tactics for \S\ref{sec:DeclarativeProof}, since we make use of some of the lower level details.

As far as tactics go, a \emph{goal} is a pair consisting of a goal formula together with \emph{hypotheses}. The purpose of a tactic is to input a goal and produce zero or more subgoals. These are then collected onto a goal stack, which acts like an agenda of problems that the user must solve. The user's aim is to apply tactics until the agenda is empty, after which, a sequent is constructed.

Many tactics correspond to inference rules but applied in reverse. For instance, there is a tactic $\code{DISCH\_TAC}$ which can be applied when one's goal formula is an implication $P \implies Q$. This deletes the current subgoal and replaces it with a goal whose formula is $Q$ but now under an additional \emph{hypothesis} $P$.

\subsection{Fully Automated Procedures}
Some tactics in HOL~Light are decision procedures or otherwise fully automated verification tools. Generally, these are used to solve a goal \emph{outright}, thus without generating any new subgoals. Among HOL~Light's decision procedures are those for checking tautologies, for deciding problems in linear arithmetic, and for computing ideal elements via Gr\"{o}bner bases~\cite{BuchbergerGrobner}.

More recently, the tools have expanded outside of ML. Eekelen et al~\cite{HOLLightBoolean} have implemented HOL~Light code to take proof certificates from external ``off-the-shelf'' checkers for universally quantified propositional logic. The certificates are processed into sequents. Similar progress has been made in HOL4~\cite{HOLBoolean} and Isabelle~\cite{IsabelleSledgehammer}.

\section{Declarative Proof}\label{sec:DeclarativeProof}
Proof assistants input verifications in broadly two forms: declarative and procedural. The difference is analogous to that between declarative and procedural programming languages. 

In a procedural language, the user employs tactics to compose automated tools in order to produce the desired sequents. Different tactic languages have their own styles and idioms, but they usually support both \emph{forward} reasoning from the premises of an argument to its conclusion, and \emph{backward} reasoning, breaking down the goal conclusion into simpler subgoals. Always the focus is on procedural \emph{transformations} rather than logical formulas, which are sometimes entirely absent from procedural verifications. 

% Consider our procedural verification of the fact that $xs ++ ys = us ++ vs \iff xs = us \wedge ys = vs$ for lists $xs$, $ys$, $us$ and $vs$ where $xs$ and $ys$ have the same length:

% \begin{scriptsize}
%   \begin{align*}
%     \code{LIST\_INDUCT\_TAC THEN } &\code{GEN\_TAC THEN LIST\_INDUCT\_TAC}\\
% \qquad \code{THEN (}&\code{REWRITE\_TAC [APPEND]}\\
% &\code{THEN REWRITE\_TAC [LENGTH;NOT\_SUC] THEN}\\
% &\code{REWRITE\_TAC [EQ\_SYM\_EQ;NOT\_SUC] THEN NO\_TAC}\\
% &\code{ORELSE ASM\_SIMP\_TAC [APPEND;CONS\_11;LENGTH;SUC\_INJ;CONJ\_ACI]))}
% \end{align*}
% \end{scriptsize}

Declarative proof assistants on the other hand, beginning with \emph{Mizar}~\cite{MizarMathematicalVernacular}, have attempted to imitate the style of ordinary mathematics. The verifications show the flow of argument as \emph{steps} which introduce intermediate formulas, with branches at subproofs and case-splits. The flow of the script always moves \emph{forward} from assumptions to conclusion, focusing on \emph{what} the logical relations between formulas are, rather than \emph{how} the internal state (the goal stack in the case of tactics) is transformed to represent such relations. This style of proof assistant relies heavily on automation, guided by how the user breaks the proof down into intermediate formulas.

Declarative style verification was a natural choice for verifying synthetic geometry. We wanted to directly compare our verifications with Hilbert's prose arguments,  so it made sense to produce verifications that matched the prose at least structurally. We would also suggest that procedural verifications, relying so often on contextual rewriting, do not work particularly well in the domain of synthetic geometry. This is a difficult claim to back up, because it might be that we could have fixed the ``problem'' with better representations. For now, we just note that the majority of our lemmas and theorems are non-equational and have large numbers of hypotheses. These are difficult to apply as contextual rewrite rules.

\subsection{Mizar~Light}\label{sec:MizarLight}
Mizar~Light, developed by Wiedijk~\cite{MizarLight}, is a declarative style language embedded in HOL~Light and inspired by the primitives of the declarative proof assistant, Mizar~\cite{MizarMathematicalVernacular}. We give an overview of its primitives in Figure~\ref{fig:MizarLight}. With the exception of \code{using}, we have emphasised a declarative semantics: rather than describing \emph{how} each primitive affects the state of the prover, we describe \emph{what} each primitive asserts at a given point in a script.

\begin{figure}
  \centering
  \begin{tabular}{|l|l|}
    \hline
    Primitive & Meaning \\
    \hline\hline
    \code{theorem} $term$ & Begins a proof of $term$. \\
    \hline
    \multirow{2}{*}\code{proof} $proof$ & Asserts $proof$ as a justification\\&for the current step. \\
    \hline
    \multirow{2}{*}\code{assume} $term$ & Asserts $term$ as a justifiable \\&assumption at this point. \\
    \hline
    \multirow{2}{*}\code{so} & Refers to the previous step as\\& justifying the current step.\\
    \hline
    \code{have} $term$ & Asserts $term$ as derivable at this point. \\
    \hline
    \multirow{2}{*}\code{thus} $term$ & Asserts $term$ as derivable at which\\&point the (sub)theorem is justified. \\
    \hline
    \code{hence} $term$ & As \code{so thus} $term$. \\
    \hline
    \multirow{2}{*}\code{take} $var$ & Identifies $var$ as the witness for the \\&(sub)theorem. \\
    \hline
    \multirow{2}{*}\code{fix} $vars$ & Establishes $vars$ as fixed but \\&arbitrary variables.\\
    \hline
    \multirow{2}{*}\code{consider} $vars$ \code{st} $term$ & Introduces $vars$ witnessing \\& $term$. \\
    \hline
    \multirow{2}{*}\code{from} $steps$ & Refers to proof steps $steps$ as \\&justifications for the current step.\\
    \hline
    \multirow{3}{*}\code{by} $thms$ & Refers to previously established theorems \\&$thms$ as justifications for the current\\&step. \\
    \hline
    \multirow{2}{*}\code{using} $tactics$ & Augments the justification of this step\\&with $tactics$.\\
    \hline
    \multirow{2}{*}\code{per cases} $cases$ & Begins a case-split into $cases$ with their\\&proofs.\\
    \hline
    \multirow{2}{*}\code{suppose} $term$ & A syntactic marker to identify the\\&supposition of each $case$. \\
    \hline
    \multirow{3}{*}\code{otherwise} $proof$ & Indicates that the (sub)theorem $thm$ can\\&be established by $proof$, which derives\\&a contradiction from $\neg thm$. \\
    \hline
    \code{set} $bindings$ & Introduces local variable bindings.\\
    \hline
    \multirow{2}{*}\code{qed} & Asserts that the (sub)theorem is justified\\&at this point.\\
    \hline
  \end{tabular}\\
  \caption{An overview of Mizar Light}
  \label{fig:MizarLight}
\end{figure}

As noticed by Harrison~\cite{MizarHOL}, the operational semantics of these primitives can be given in terms of tactics and simplified goals. The tree of goal-stacks becomes a tree of subproofs and case-splits. The hypotheses of each goal become the intermediate lemmas. Each step becomes a tactic which drives the verification \emph{forward}. Here, for instance, is a typical step one might read in one of our Mizar~Light verifications:
\begin{center}
\code{consider}\ $P$ \code{such that}\ $\neg$\code{on\_line}\ P\ a \code{by}\ \eqref{eq:g12},\eqref{eq:g13a}.
\end{center}
This $\code{consider}$ step translates to a tactic which introduces a subgoal with term\linebreak \code{$\exists$P. $\neg$on\_line P a}. The goal is solved outright using the step's \emph{justification}. By default, steps are justified by HOL~Light's generic \code{MESON} tactic~\cite{HarrisonMESON}, but additional tactics can be composed with the \code{using} keyword. The justification tactic usually needs some help to solve its goals, and in declarative verification, we name justifying sequents using the keyword \code{by}. In this example, we have added some sequents (\ref{eq:g12} and \ref{eq:g13a}) which are passed directly to \code{MESON}.

All Mizar~Light primitives are ordinary ML functions, and the Mizar~Light language is really just a combinator language~\cite{CombinatorLanguages}. This has been useful to us, since it is almost trivial to modify and extend Mizar~Light by writing new combinators.

That combinators make it easy to extend a system is a feature we find particularly attractive. Tactics themselves are implemented as combinators, which makes it easy for users to write their own (we describe a few of ours in Chapters~\ref{chapter:Automation} and~\ref{chapter:LinearOrder}). Another feature of tactics is that they compose algebraically. There are tensoring operators, sums, identities and a zero. We have tried to keep to this spirit of algebraic combinator languages in our own automation, described in Chapter~\ref{chapter:Automation}.

\subsection{Extending Mizar~Light for Interactivity}\label{sec:MizarLightExtend}
Wiedijk's basic combinators are based on the original Mizar system, a batch prover, and in this spirit, Mizar~Light verifications are written in their entirety and then evaluated in one. We found this undesirable, firstly, because the error reporting is not rich enough to show where errors occur in the case of a failed verification. Secondly, we chose to implement our automation (described in Chapter~\ref{chapter:Automation}) so that it ran concurrently as we developed our verification. Here, the tool works best when it can exploit our idle time when working \emph{interactively} as opposed to \emph{batch} mode.

The problem lies with case-splitting. Here are the original combinators at work in an extract of one of Wiedijk's example verifications (the details of which are not important):

\vspace{0.5cm}
\begin{minipage}{\linewidth}
  \footnotesize
  \code{...}

  \code{have "$\forall$p1 p2. $\exists$l. p1 ON l $\wedge$ p2 ON l" at 9}

  \code{proof}

  \code{\enspace [fix ["p1:Point"; "p2:Point"];}

  \code{\quad per cases}

  \code{\quad\enspace[[suppose "p1 = p2";}

  \code{\qquad\enspace qed from [0] by [LEMMA1]];}

  \code{\qquad [suppose "$\neg$(p1 = p2)";}

  \code{\qquad\enspace qed from [1]]]];}

  \code{...}
\end{minipage}
\vspace{0.5cm}

Within this verification is nested a case-split on the formulas \code{p1 = p2} and \linebreak\code{$\neg$(p1 = p2)}. The steps in each case are collected in lists, which makes for a neatly structured verification, where trees of subproofs and case-splits are reflected by ML data-structures. However, the steps of an interactive verification are supposed to be applied \emph{linearly}, one-by-one, traversing an implicit tree. Here is what we prefer to write at the top-level (\code{>} marks the ML prompt):

\vspace{0.5cm}
\begin{minipage}{\linewidth}
  \footnotesize
  \code{> have "$\forall$p1 p2. $\exists$l. p1 ON l $\wedge$ p2 ON l" at 9}

  \code{> proof}

  \code{> fix ["p1:Point"; "p2:Point"]}

  \code{> per cases}

  \code{> suppose "p1 = p2"}

  \code{> qed from [0] by [LEMMA1]}

  \code{> suppose "$\neg$(p1 = p2)"}

  \code{> qed from [1]}
\end{minipage}
\vspace{0.5cm}

We could have probably achieved this linearisation by rewriting Wiedijk's proof system in a continuation-passing style. However, the development of HOL~Light has emphasised backward-compatibility, and so we built on the existing implementation.

\subsubsection{Interactive Case-splits}
Case splits are ultimately justified by proving a disjunction of all considered cases. However, in the Mizar-style verification and as is common in ordinary mathematical proof, the particular disjunction is never stated explicitly. Consider again the extract of Mizar Light code:

\vspace{0.5cm}
\begin{minipage}{\linewidth}
  \footnotesize
  \code{\quad per cases}

  \code{\quad\enspace[[suppose "p1 = p2";}

  \code{\qquad\enspace qed from [0] by [LEMMA1]];}

  \code{\qquad [suppose "$\neg$(p1 = p2)";}

  \code{\qquad\enspace qed from [1]]]];}
\end{minipage}
\vspace{0.5cm}

{\samepage Here, there are two cases being considered \code{p1 = p2} and \code{$\neg$(p1 = p2)}. The disjunction which justifies them as exhaustive
\begin{center}\code{p1 = p2 $\vee$ $\neg$(p1 = p2)}\end{center}}

\noindent does not appear in the verification. Instead, it is assembled by the \code{per cases} step by gathering the \code{suppose} formulas, before the tactics for each case are ever applied. This is possible, because \code{per cases} takes the full list of cases, from which the disjunction can be assembled. But this strategy will not work if we are to linearise the subproofs and apply each step interactively, since the full disjunction will not be known until all cases are interactively solved.

Harrison's original Mizar mode for HOL Light had better support for interactive case-splitting~\cite{MizarHOL}. The drawback appears to be that it relies on potentially invalid tactics, being tactics which succeed but from which a final verification cannot be recovered. Our implementation is different. We use two functions \code{case} and \code{end}. The \code{case} function is used to introduce a new case term $P$. It then generates two subgoals, the first with $P$ as hypothesis, and the second with $\neg P$ as hypothesis. The \code{case} step, therefore, has performed a case-split on $P\vee\neg P$. The user must first prove the goal on the hypothesis of $P$. Once the goal is solved, the one remaining goal will have $\neg P$ as its hypothesis.

The user now proceeds by introducing the \emph{next} case, using the \code{case} function again with a new term, say $Q$. Two subgoals are again generated, one with $Q$ and the other with $\neg Q$ as hypothesis.

By the time the user has considered and proven all cases, the one remaining subgoal will have the negations of every considered case in its hypotheses. If the cases are exhaustive, the negations will entail a contradiction\footnote{We assume we are only interested in \emph{classical} proofs. Otherwise, this does not necessarily follow.}. This is where the \code{end} step is used. It will automatically take all the negated cases, identifying them by a case-label \code{Case} in the goal-stack, and use them as a justification for $\bot$.

Suppose, for example, that we have a sequent $\vdash P \vee Q \vee R$ assigned to $\phi$ and suppose that a goal with term $G$ can be solved on each of the hypotheses $P$, $Q$ and $R$ using just the implicit automation built into Mizar~Light. Then we can write the verification:

\vspace{0.5cm}
\begin{minipage}{\linewidth}
  \footnotesize
  \code{> theorem "$G$"}

  \code{> case "$P$" }

  \code{>\quad qed}

  \code{> case "$Q$"}

  \code{>\quad qed}

  \code{> case "$R$"}

  \code{> end by $\phi$}
\end{minipage}
\vspace{0.5cm}

The resulting tree of goal-stacks is depicted Figure~\ref{fig:CaseProofTree}.

\begin{figure}
\begin{center}
\includegraphics{background/ProofTree}
\end{center}
\caption{Case-splitting Proof Tree}
\label{fig:CaseProofTree}
\end{figure}

The final \code{end} step is justified since
\begin{displaymath}
P \vee Q \vee R, \neg P, \neg Q, \neg R \vdash \bot
\end{displaymath}

This approach only uses valid tactics. Whenever the case splitting is not exhaustive, the \code{end} step will immediately fail.

Returning to our example verification, our functions \code{case} and \code{end} allow us to write

\vspace{0.5cm}
\begin{minipage}{\linewidth}
  \footnotesize
  \code{> lemma "$\forall$p1 p2. $\exists$l. p1 ON l $\wedge$ p2 ON l" at 9}

  \code{>\quad fix ["p1:Point"; "p2:Point"]}

  \code{>\quad case "p1 = p2" }

  \code{>\qquad qed from [0] by [LEMMA\_1] }

  \code{>\quad case "$\neg$(p1 = p2)" }

  \code{>\qquad qed from [1]}

  \code{>\quad end}
\end{minipage}
\vspace{0.5cm}

\subsection{Concluding Remarks}
Allowing interactive case-splitting worked well in practice. We would write our verifications interactively, and when completed, package them up as batch verifications. The translation between flattened verification and the normal batch \code{per cases} combinator was always straightforward.

Further modifications to Mizar~Light are described in Chapter~\ref{chapter:Automation}.

\section{Conventions}
As with any thesis of this sort, there are certain ambiguities which arise from overloading and which ought to be clarified. We will adopt some terminology.

We use ``proposition'' to refer to ordinary mathematical results given in natural language and proven in prose. When we refer specifically to propositions given by Hilbert in the \emph{Grundlagen der Geometrie}, we shall use the uppercase ``THEOREM''. We then reserve the words ``theorem'' and ``lemma'' for sequents $\vdash t$ produced in HOL~Light where $t$ formalises a natural language theorem. When we talk of theorems and lemmas from here on, we are referring to mechanically verified propositions. Finally, the words ``verification'' and ``formal proof'' will be used variously for the proof scripts we use to produce our theorems.

Throughout the thesis, we shall give sample verifications. These depict actual Ocaml code, but for readability, we elide various pieces of syntax such as brackets and semi-colons. We also introduce our own syntactic sugar, shown in the following table.

\begin{center}\label{sec:Translations}
  \begin{tabular}{|c|c|}
    \hline
    Notation   & Translation \\
    \hline
    $\neg$     & \code{\tt\char`~} \\
    $\wedge$   & \code{/{\tt\char`\\}} \\
    $\vee$     & \code{{\tt\char`\\}/} \\
    $\implies$ & \code{==>} \\
    $\iff$     & \code{<=>} \\
    $\forall$  & \code{!} \\
    $\exists$  & \code{?} \\
    $\lambda x.$  & \code{\tt\char`\\ x.} \\
  $P \neq Q$ & \code{\tt\char`~(P=Q)}\\
  $\alpha$, $\beta$, $\gamma$ & \code{'a}, \code{'b}, \code{'c}\\
  $x,y \in A,B$ & \code{x IN A /{\tt\char`\\} y IN A /{\tt\char`\\} x IN B /{\tt\char`\\} y IN A}\\
  \hline
  \end{tabular}
\end{center}

The Mizar~Light language uses expressions of the form $\code{at } m$ to label an intermediate result with the number $m$. This number is then used by the combinator $\code{from}$ to justify later steps by cross-referencing. 

We have changed the implementation slightly so that the $\code{at}$ combinator takes a list. Now an expression such as $\code{at [}m\code{, } n\code{, } p\code{]}$ can be used to label the individual conjuncts (in this case, three of them) of an intermediate step, each of which can be cross-referenced separately. In the sample verifications in this thesis, we shall elide the combinator $\code{at}$ and instead set $m$, $n$ and $p$ as right-justified labels in the verification. 

Many of our ideas evolved during the verification development, and, as of writing, there are legacy naming conventions which need to be refactored and brought into line with the present thesis. For now, we can only claim that the verifications reproduced in the thesis are $\alpha$-equivalent to the originals.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
