\chapter{Conclusion}\label{chapter:Conclusion}
In an article published in the \emph{American Mathematical Monthly}~\cite{GuggenheimerJordanProof}, on the validity of the Polygonal Jordan Curve Theorem in ordered geometry, almost forgotten, Guggenheimer laments:
\begin{quotation}
  ``It is astonishing that none of the textbooks of elementary axiomatic geometry gives a proof.''
\end{quotation}

We find it more astonishing considering that the axioms of ordered geometry mark the first major contribution to the modern rigorisation of geometry by Pasch, ``the father of rigour in geometry''. It stretches all our credibility when those same axioms were delineated in the most influential textbook on modern axiomatic geometry, nearly eight decades earlier,  and the theorem stated as derivable, by David Hilbert, one of the greatest mathematicians who ever lived.

Such is hindsight. We confess that we had little interest in the theorem initially. Hilbert implied it was trivial, that, in the \emph{Grundlagen}, the interesting proofs only appear after the later groups of axioms. If not for the demands of formal verification, we would have missed the literal labyrinths that were left unexplored in the wake of his first two groups. We would have almost certainly glossed over the details, letting ourselves be convinced by invalid proofs --- Veblen's, our own, and whatever Hilbert had in mind when he declared the result obtainable without much difficulty. Had it not been for formal verification, we would not have done justice to ordered geometry.

However, the substantial effort required of a formal verification detracts from these benefits. With current technology, simply translating Hilbert's prose steps to formal logic and expecting generic automated tools to fill in all the logical inferences from a small logical kernel is infeasible. The automation cannot fill the gaps in a timely fashion, and it might even be that it would still ultimately fail given unlimited time as it wasted its limited memory with doomed search strategies.

The problem, we conjecture, is that we are working in a domain which was synthetic, not algebraic, and therefore not amenable to equational reasoning, where powerful term rewriting systems which are a workhorse of automated theorem proving can be brought to bear on the problems. Instead, we have implicational theorems with complex antecedents, whose variables must be carefully instantiated and whose conclusions can then be combined to find the correct path to the goal.

By Hilbert's geometry faithfully, we were committed to the synthetic style of proof, but we were not happy manually adding the excruciatingly detail needed to fill the gaps in the arguments. Doing so leaves the verifications bloated, and they a pale imitation of the original prose and the geometric insight that produced it. This is not a desirable state of affairs.

As we saw in Chapter~\ref{chapter:Group2Eval}, our domain \emph{was} amenable to tailored proof-search. Since the axioms needed to fill the proof gaps almost never introduce points into a geometric configuration, we can exhaustively search for all the incidence relations that hold in a figure. Restricting our attention to a small finite geometry in this way, characterised by sets of collinear, non-collinear and planar points, and the rules between them, we are faced with a tractable combinatorial space.

It is possible that there are other theories that have this feature, and which have so far resisted an effective formalisation for lack of tool support. We hope that our new automation can be of assistance here, as we were careful to design it in a generic way, supporting extension and composition across domains. To that end, we followed the example of traditional theorem proving tools, namely tactics and conversions~\cite{Tactics}, and defined a combinator language for proof search. This language allowed us to rapidly prototype a search strategy specific to incidence reasoning, in a way which is naturally extensible and composable across different domains, 

A crucial assumption of our search language is that the data found (such as sequents about which sets of points are collinear, or which angles are equal) grows cumulatively. Later data should not replace earlier data, and data should not be discarded as new information comes in. This is not generally a realistic assumption, and already posed some inefficiences for us when we came to think about point equalities, which should ideally force all prior data to be normalised. However, despite the inefficiencies, the tool remained highly effective.

With automation, we typically found that our new verifications corresponded one-to-one with the prose. That is, our verifications were structured just as if the formal steps were direct translations of steps in the prose argument, the ideal of declarative proofs. It is as if we had managed to recreate the silent mechanical reasoning capacity of an artificial Hilbert, going some way to vindicate the great mathematician's commitment to axiomatic rigour.

It was crucial to have this automation by the time we came to verify the Polygonal Jordan Curve Theorem. Typically, those working in formal verification transcribe proofs that they know to be basically correct. We, on the other hand, were trying to patch a proof that we were convinced was wrong, and the doubts surrounding it from accomplished mathematicians such as Lennes We only dared the venture because we were confident that our incidence automation made it feasible. Otherwise, we could scarcely imagine how we would have completed it. The verification consisted of roughly 1400 declarative proof steps. Of these, 213 are simple \code{assume} steps that come with no justification. Another 215 are \code{consider} steps which introduce geometric entities. 116 are \code{fix} steps for naming the quantified variables. Of the remaining 849 intermediate steps, 111 were handled by the incidence automation of Chapter~\ref{chapter:Automation} and 82 by the combination of incidence automation and linear reasoning automation of Chapter~\ref{chapter:LinearOrder}. Going by the results of Chapter~\ref{chapter:Group2Eval}, we may have replaced over 500 proof steps about tedious incidence reasoning alone.

These are steps that had been difficult to obtain by hand. Some we had missed entirely, something we could expect to happen frequently on the substantially larger verification of the Polygonal Jordan Curve Theorem, leading to mental blocks which may then have sent us down spurious paths, or worse, had us give up entirely. But with the incidence details taken care of, our proofs could focus on the less combinatorial steps, those that interest a model geometer, and which can be sketched out on paper and subjected to the full resources of intuitive observation.

One lesson we would take from the need and efficacy of our automation is that the ideal of declarative verification, where the user only has to think in terms of lemmas and their dependencies, letting generic automation connect the dots, is probably going to be overly optimistic for some time to come. Domain specific automation, crafted by the user, will still be needed, and those working in formal verification must continue to think as computer scientists and programmers. Theorem provers must continue to cater to their needs, as HOL~Light does, putting its users directly at the interpreter of a powerful programming language conceived with the explicit aim of writing proofs.

We would like to conclude by reflecting a little on the experience verifying the Polygonal Jordan Curve Theorem. We believe that formal verification holds a unique place in the general space of human problem solving. The chief point is that, in a verification, the goal is stated up front without ambiguity, and the circumstances under which that goal is achieved are unambiguous, authoritative and not subject to any human review. The sense of success in solving a problem is rarely so definitive and clear cut.

We had the problem stably formalised very early on, and so the prospect of its solution was almost tangible. We just needed the theorem prover to produce the formalised problem statement as a fully fledged theorem, curtly announcing ``No subgoals''. However, we did not anticipate the effort required to get there. This was not because the verification was a slog, for our automation took care of the pedantic mechanical details. It was just that the problem seemed to grow in complexity as we tackled it, and the theorem prover could not give us a reliable measure of our distance from the goal.

We could sweat out a major lemma such as \ref{eq:changeTriangle} from Appendix~\ref{app:JordanVerificationExtra}, but with theorems such as these, which have half a dozen opaque hypotheses and are based on admittedly convoluted definitions, we found it difficult to convince ourselves that the theorems said exactly what we wanted them to say and that they put us on the right track. 
At any time, we were prepared for the possibility that we were navigating ourselves into a dead-end, having misunderstood some crucial aspect of the problem. And struggling to find a sense of being on the true course and making genuine progress, we found it even more difficult to communicate a sense of progress to others. We were alone in this maze.

So when the theorem prover finally and curtly announced ``No subgoals'', it was like turning a corner in the labryinth and then emerging disoriented in bright daylight. There was no need to trace back over our tracks, scrutinising and carefully reassuring ourselves that we had indeed found the exit. That instant, we knew that we had solved the problem that had started it all, that our strategy was automatically vindicated and any previous uncertainties dissolved.

In practised mathematics, a proof must convince human readers. The subtle complexities of its solution must be teased out and magnified lest they mislead us into fallacy. But a verification is wholly unlike this. Already convinced that our verifier is sound, and having been long convinced that the relatively short problem statement is the one we intend, the verification is complete the moment the computer declares it so. Looking back over the verification, we can start accepting the subtleties with some suspension of disbelief, and not feel obliged to investigate every last nook and cranny: the machine has vouched for us. The path of definitions and lemmas can instead be left as a story offering only insight on the complexities involved, along with some nuggets of useful techniques and strategies for other adventurers. The possibility of such a profound \emph{fait accompli} is a rare treasure in itself.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
